{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fee55ab7-776a-4bbe-8fc4-258f74fb5ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import gym\n",
    "import highway_env\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "import torch.nn.functional as F\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7763902a-c0b4-48a0-bc1e-c53e01da1299",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        state = np.expand_dims(state, 0)\n",
    "        next_state = np.expand_dims(next_state, 0)\n",
    "\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        state, action, reward, next_state, done = zip(\n",
    "            *random.sample(self.buffer, batch_size))\n",
    "        return np.concatenate(state, axis=0), action, reward, np.concatenate(next_state), done\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b6ac39bd-307c-4607-9e81-b22b6beaf437",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_actions = num_actions\n",
    "\n",
    "        self.feature = nn.Sequential(\n",
    "            nn.Linear(self.num_inputs, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.advantage = nn.Sequential(\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, self.num_actions)\n",
    "        )\n",
    "\n",
    "        self.value = nn.Sequential(\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        f = self.feature(x)\n",
    "        advantage = self.advantage(f)\n",
    "        value = self.value(f)\n",
    "        return value + advantage - advantage.mean()\n",
    "\n",
    "    def act(self, state, epsilon):\n",
    "        if random.random() > epsilon:\n",
    "            state = torch.FloatTensor(state).unsqueeze(0)\n",
    "            q_value = self.forward(state)\n",
    "            action = q_value.argmax()\n",
    "            action = action.item()\n",
    "        else:\n",
    "            action = random.randrange(self.num_actions)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "98626239-1c36-4530-ba35-d491c1aaa753",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dueling_DQN:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def init_hyp(self):\n",
    "        self.epsilon_start = 1.0\n",
    "        self.epsilon_final = 0.01\n",
    "        self.epsilon_decay = 500\n",
    "        self.epsilon_by_frame = lambda frame_idx: self.epsilon_final + \\\n",
    "            (self.epsilon_start - self.epsilon_final) * \\\n",
    "            math.exp(-1. * frame_idx / self.epsilon_decay)\n",
    "        self.num_frames = 10000\n",
    "        self.batch_size = 32\n",
    "        self.gamma = 0.99\n",
    "        self.losses = []\n",
    "        self.all_rewards = []\n",
    "        self.episode_reward = 0\n",
    "        self.checkpoint_path = \"checkpoints/dueling-dqn-highway.pth\"\n",
    "        self.do_render = True\n",
    "        self.do_show_log = True\n",
    "\n",
    "    def init_env(self):\n",
    "        env_id = \"highway-v0\"\n",
    "        self.env = gym.make(env_id)\n",
    "\n",
    "        print(\"obs space = {} action space = {} \".format(self.env.observation_space.shape,\n",
    "              self.env.action_space.n))\n",
    "\n",
    "    def create_model(self):\n",
    "        self.current_model = Model(\n",
    "            np.prod(self.env.observation_space.shape), self.env.action_space.n)\n",
    "        self.target_model = Model(\n",
    "            np.prod(self.env.observation_space.shape), self.env.action_space.n)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.current_model.parameters())\n",
    "        self.replay_buffer = ReplayBuffer(1000)\n",
    "        self.update_target(self.current_model, self.target_model)\n",
    "        self.save_model()\n",
    "\n",
    "    def update_target(self, current_model, target_model):\n",
    "        self.target_model.load_state_dict(self.current_model.state_dict())\n",
    "\n",
    "    def compute_td_loss(self, batch_size):\n",
    "        state, action, reward, next_state, done = self.replay_buffer.sample(\n",
    "            batch_size)\n",
    "\n",
    "        state = torch.FloatTensor(np.float32(state))\n",
    "        next_state = torch.FloatTensor(np.float32(next_state))\n",
    "        action = torch.LongTensor(action)\n",
    "        reward = torch.FloatTensor(reward)\n",
    "        done = torch.FloatTensor(done)\n",
    "\n",
    "        q_values = self.current_model(state)\n",
    "        next_q_values = self.target_model(next_state)\n",
    "\n",
    "        q_value = q_values.gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "        next_q_value = next_q_values.max(1)[0]\n",
    "        expected_q_value = reward + self.gamma * next_q_value * (1 - done)\n",
    "\n",
    "        loss = (q_value - expected_q_value.data).pow(2).mean()\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def plot(self, frame_idx, rewards, losses):\n",
    "        clear_output(True)\n",
    "        plt.figure(figsize=(20, 5))\n",
    "        plt.subplot(131)\n",
    "        plt.title('frame %s. reward: %s' % (frame_idx, np.mean(rewards[-10:])))\n",
    "        plt.plot(rewards)\n",
    "        plt.subplot(132)\n",
    "        plt.title('loss')\n",
    "        plt.plot(losses)\n",
    "        plt.show()\n",
    "\n",
    "    def plot_epsilon():\n",
    "        plt.plot([self.epsilon_by_frame(i) for i in range(10000)])\n",
    "\n",
    "    def train(self):\n",
    "        state = self.env.reset()\n",
    "        for frame_idx in range(1, self.num_frames + 1):\n",
    "            epsilon = self.epsilon_by_frame(frame_idx)\n",
    "            action = self.current_model.act(state, epsilon)\n",
    "\n",
    "            next_state, reward, done, _ = self.env.step(action)\n",
    "            if self.do_render:\n",
    "                self.env.render()\n",
    "            self.replay_buffer.push(state, action, reward, next_state, done)\n",
    "\n",
    "            state = next_state\n",
    "            self.episode_reward += reward\n",
    "\n",
    "            if done:\n",
    "                state = self.env.reset()\n",
    "                self.all_rewards.append(self.episode_reward)\n",
    "                self.episode_reward = 0\n",
    "\n",
    "            if len(self.replay_buffer) > self.batch_size:\n",
    "                loss = self.compute_td_loss(self.batch_size)\n",
    "                self.losses.append(loss.item())\n",
    "\n",
    "            if frame_idx % 200 == 0 and self.do_show_log:\n",
    "                self.plot(frame_idx, self.all_rewards, self.losses)\n",
    "                print(\"average reward = {}\".format(np.mean(self.all_rewards[-10:])))\n",
    "\n",
    "            if frame_idx % 100 == 0:\n",
    "                self.update_target(self.current_model, self.target_model)\n",
    "                self.save_model()\n",
    "\n",
    "    def save_model(self):\n",
    "        torch.save(self.target_model, self.checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f8fe330-86eb-4fc7-bb48-eadfdf39d580",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = Dueling_DQN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "55f29159-9196-40e3-abb9-b37242939277",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "obs space = (5, 5) action space = 5 \n"
     ]
    }
   ],
   "source": [
    "obj.init_hyp()\n",
    "obj.init_env()\n",
    "obj.create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71918060-8446-4648-b7d1-b539ee2bced5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standalone function evaluate model on the environment\n",
    "def evaluate():\n",
    "    checkpoint_path = \"checkpoints/dueling-dqn-highway.pth\"\n",
    "    model = torch.load(checkpoint_path)\n",
    "    env = gym.make(\"highway-v0\")\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while done != True:\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        q_value = model(state)\n",
    "        action = q_value.argmax()\n",
    "        action = action.item()\n",
    "        state, reward, done, info = env.step(action)\n",
    "        env.render()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef0512cb-7783-4d4d-88a4-1d547a66e061",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237ec3b7-ce5d-47f7-a9e3-dc7371a46b14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531974fe-ce5e-4a21-b909-7aaa803071e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a5d7a8-b319-401a-a01d-1d1aa7f0d4d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19265a64-7ce7-4197-a91b-55653af97662",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433db1d4-dd55-4180-8031-a2dae89ec682",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}